{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pydpp.dpp import DPP\n",
    "import numpy.linalg as lg\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeLogLikeLihood(LowRankMatrix,nb_samples,nb_items,nb_trait,training_samples,lambdaVec,alpha):\n",
    "    #first term of the LL\n",
    "    L=LowRankMatrix@LowRankMatrix.T\n",
    "    sumDetFirstTerm=0\n",
    "    Log_Determinant_LN=0\n",
    "    for i in range(nb_samples):\n",
    "        V_n=np.take(LowRankMatrix,training_samples[i], axis=0)\n",
    "        #V_n=LowRankMatrix[training_samples[i],:]\n",
    "        #print(V_n.shape)\n",
    "        L_n=V_n@V_n.T\n",
    "        #print(L_n.shape[0])\n",
    "        if L_n.shape[0] ==1 :\n",
    "            determinant_Ln=0\n",
    "        else :\n",
    "            determinant_Ln=lg.det(L_n)\n",
    "        if determinant_Ln>0:\n",
    "            Log_Determinant_LN=math.log(determinant_Ln)\n",
    "        sumDetFirstTerm+=Log_Determinant_LN\n",
    "    firstTerm=sumDetFirstTerm\n",
    "    #second term of LL\n",
    "    DetSecTerm=lg.det(np.identity(nb_items)+L)\n",
    "    if DetSecTerm>0:\n",
    "        LogSecTerm=math.log(DetSecTerm)\n",
    "        \n",
    "    SecondTerm=nb_samples*LogSecTerm\n",
    "    \n",
    "    #Third term ( regularization)\n",
    "\n",
    "    Third1=0\n",
    "    for i in range(nb_items):\n",
    "        #print(lambdaVec[i])\n",
    "        lmbd=lambdaVec[i]\n",
    "        norm=lg.norm(LowRankMatrix[i,:], 2)\n",
    "        thirdd=np.round(lmbd,5)*np.round(norm,5)\n",
    "        Third1+= thirdd\n",
    "    ThirdTerm=0\n",
    "    if alpha!=0:\n",
    "        ThirdTerm=alpha*Third1\n",
    "    \n",
    "    #compute the LL\n",
    "    \n",
    "    LL=firstTerm-SecondTerm-0.5*ThirdTerm\n",
    "    \n",
    "    return LL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeGradient(LowRankMatrix,nb_samples,nb_items,nb_trait,training_samples,lambdaVec,alpha):\n",
    "    M,K=LowRankMatrix.shape #utiliser nb_items et nb_trait !\n",
    "    sumTraceFirstTermGradient=0\n",
    "    Gradient_Matrix=np.zeros((M,K))\n",
    "    VMatrix_NinstanceVector=[]\n",
    "    LMatrix_Ninstance_InverseVector=[]\n",
    "    Size_V_NinstanceVector=[]\n",
    "    #Martix of all V[samples] and L[samples]inverse\n",
    "    #first term\n",
    "    for i in range(nb_samples):\n",
    "        V_Ninstance=LowRankMatrix[training_samples[i],:]\n",
    "        VMatrix_NinstanceVector.append(LowRankMatrix[training_samples[i],:])\n",
    "        \n",
    "        LMatrix_Ninstance=V_Ninstance@V_Ninstance.T\n",
    "        \n",
    "        LMatrix_Ninstance_Inverse=lg.pinv(LMatrix_Ninstance)\n",
    "        \n",
    "        LMatrix_Ninstance_InverseVector.append(LMatrix_Ninstance_Inverse)\n",
    "        \n",
    "        Size_V_NinstanceVector.append(V_Ninstance.shape[0])\n",
    "    \n",
    "    \n",
    "    #second term\n",
    "    pre_secondTerm=np.identity(M)-LowRankMatrix@(lg.pinv(np.identity(K)+(LowRankMatrix.T)@LowRankMatrix))@LowRankMatrix.T\n",
    "    \n",
    "      \n",
    "    ## compute the gradient for F(i,k)\n",
    "    BuildMAP=Build_Map_Training_RowCol(training_samples,nb_items,nb_samples)\n",
    "    for k in range(K):\n",
    "        for i in range(M):\n",
    "            SumTraceFirstTerm=0\n",
    "            SumTraceSecondTerm=0\n",
    "            for l in range(nb_samples):\n",
    "                A=LMatrix_Ninstance_InverseVector[l]\n",
    "                V=VMatrix_NinstanceVector[l]\n",
    "                size=Size_V_NinstanceVector[l]\n",
    "                instance=BuildMAP[l,i]\n",
    "                if instance !=0 :\n",
    "                    itemNotPresent=False\n",
    "                else :\n",
    "                    itemNotPresent=True\n",
    "                traceFirst=0\n",
    "                if itemNotPresent:\n",
    "                    traceFirst=0\n",
    "                else :\n",
    "                    sumFirst=0\n",
    "                    for j in range(size):\n",
    "                        sumFirst+=A[j,int(instance)]*V[j,k]\n",
    "                    traceFirst=sumFirst + A[int(instance),:]@V[:,k]\n",
    "                \n",
    "                SumTraceFirstTerm+=traceFirst\n",
    "                \n",
    "            \n",
    "            SumAdotV2=0\n",
    "            B=pre_secondTerm\n",
    "            for j in range(M):\n",
    "                SumAdotV2+=B[j,i]*LowRankMatrix[j,k]\n",
    "            \n",
    "            SumTraceSecondTerm=B[i,:]@LowRankMatrix[:,k]+SumAdotV2\n",
    "            \n",
    "            ##gradient a l indice i,k\n",
    "            FinalTerm=SumTraceFirstTerm-nb_samples*SumTraceSecondTerm-alpha*lambdaVec[i]*LowRankMatrix[i,k] \n",
    "            \n",
    "            Gradient_Matrix[i,k]=FinalTerm\n",
    "            \n",
    "    return Gradient_Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def StochasticGradientAscent(training_samples,nb_samples,nb_items,nb_trait,max_iteration,lambdaVec,alpha):\n",
    "    M,K=nb_items,nb_trait\n",
    "    gradient=np.zeros((M,k))\n",
    "    delta=np.zeros((M,K))\n",
    "    pourc_test=0.3\n",
    "    pourc_valid=0.1\n",
    "    Eps0=1e-5\n",
    "    T=60\n",
    "    Beta=0.95\n",
    "    counter=0\n",
    "    Epsilon=Eps0/(1+counter/T)\n",
    "    split_train=int(nb_samples*0.7)\n",
    "    split_valid=int(nb_samples*0.1)\n",
    "    InitialParam=np.random.uniform(0,1,(M,K))\n",
    "    minibatch=20\n",
    "    curr_index=0\n",
    "    #training=np.random.shuffle(training_samples)\n",
    "    #traininf=map(numpy.random.shuffle, training_samples)\n",
    "\n",
    "    Train=training_samples[:split_train]\n",
    "    Valid=training_samples[split_train:split_train+split_valid]\n",
    "    Test=training_samples[split_train+split_valid:]\n",
    "    nb_train=len(Train)\n",
    "    nb_test=len(Test)\n",
    "    nb_valid=len(Valid)\n",
    "    LowRankMatrix=InitialParam\n",
    "    valid_LL_first=computeLogLikeLihood(LowRankMatrix,nb_valid,nb_items,nb_trait,Valid,lambdaVec,alpha)\n",
    "    #LogLike_valid=0\n",
    "    while counter< max_iteration :\n",
    "        \n",
    "        \n",
    "        nb_instance_Batch=minibatch\n",
    "        \n",
    "        if curr_index+minibatch> nb_train :\n",
    "            nb_instance_Batch=nb_train-curr_index\n",
    "        \n",
    "        train_batch=Train[curr_index:curr_index+minibatch]\n",
    "        \n",
    "        \n",
    "        gradient=computeGradient(LowRankMatrix+Beta*delta,nb_instance_Batch,M,K,train_batch,lambdaVec,alpha)\n",
    "        \n",
    "        delta=Beta*delta + (1-Beta)*Epsilon*gradient \n",
    "        \n",
    "        LowRankMatrix=LowRankMatrix+delta\n",
    "        \n",
    "        LL_train=computeLogLikeLihood(LowRankMatrix,nb_train,M,K,Train,lambdaVec,alpha)\n",
    "        \n",
    "        LL_valid=computeLogLikeLihood(LowRankMatrix,nb_valid,M,K,Valid,lambdaVec,alpha)\n",
    "\n",
    "        LL_test=computeLogLikeLihood(LowRankMatrix,nb_test,M,K,Test,lambdaVec,alpha)\n",
    "                \n",
    "        print(\"LogLikelihood for the training\",LL_train)\n",
    "        print(\"LogLikelihood for the test\",LL_test)\n",
    "        print(\"LogLikelihood for the valid\",LL_valid)\n",
    "        print(counter)\n",
    "        counter+=1\n",
    "\n",
    "        \n",
    "        if np.abs(valid_LL_first-LL_valid) < 1e-6:\n",
    "            break\n",
    "        else :\n",
    "            valid_LL_first=LL_valid\n",
    "        \n",
    "        curr_index+=nb_instance_Batch +1\n",
    "        \n",
    "        if curr_index> nb_train:\n",
    "            #wwe processed all the samples, start from the beginning\n",
    "            curr_index=1\n",
    "            #shuffle the data !!\n",
    "            \n",
    "        \n",
    "    return LowRankMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_LambdaVec(training_samples,nb_samples,nb_items):\n",
    "    lambdaVec=np.zeros((10,1))\n",
    "    for i in range(nb_items):\n",
    "        counter=0\n",
    "        item=i\n",
    "        for j in range(nb_samples):\n",
    "            sample=training_samples[j]\n",
    "            for k in range(len(sample)):\n",
    "                if item==sample[k]:\n",
    "                    counter+=1\n",
    "        try:            \n",
    "            lambdaVec[i]=1/counter\n",
    "        except ZeroDivisionError:\n",
    "            lambdaVec[i]=0\n",
    "    vec=[]\n",
    "    for lm in lambdaVec:\n",
    "        vec.append(float(lm))\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def Build_Map_Training_RowCol(training_samples,nb_items,nb_samples):\n",
    "    Build_Map=np.zeros((nb_samples,nb_items))\n",
    "    for i in range(nb_samples):\n",
    "        trainingInstanceItems=training_samples[i]\n",
    "        \n",
    "        for j in range(len(trainingInstanceItems)):\n",
    "            \n",
    "            Build_Map[i,trainingInstanceItems[j]]=j\n",
    "    \n",
    "    return Build_Map    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_iter=10\n",
    "X=[]\n",
    "Matrix=[]\n",
    "for i in range(10):\n",
    "    x = [np.random.randint(0, 9) for p in range(0, 50)]\n",
    "    X.append(x)\n",
    "X=np.asarray(X)\n",
    "    \n",
    "dpp = DPP(X)\n",
    "dpp.compute_kernel(kernel_type='cos-sim')\n",
    "idx = dpp.sample_k(5)\n",
    "\n",
    "for k in range(nb_iter):\n",
    "\n",
    "    for j in range(10):\n",
    "        ran=np.random.randint(2,10)\n",
    "        idx = dpp.sample_k(ran)\n",
    "    \n",
    "        Matrix.append(X[idx,j])\n",
    "        \n",
    "    k+=1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_samples=Matrix\n",
    "nb_items=10\n",
    "nb_trait=5\n",
    "M,K=10,5\n",
    "nb_samples=100\n",
    "vec=compute_LambdaVec(training_samples,nb_samples,nb_items)\n",
    "alpha=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogLikelihood for the training -980.3976989863137\n",
      "LogLikelihood for the test -147.39277183576274\n",
      "LogLikelihood for the valid -206.44515959785525\n",
      "0\n",
      "LogLikelihood for the training -431.30654362073693\n",
      "LogLikelihood for the test -147.3849883940693\n",
      "LogLikelihood for the valid -94.75545388419236\n",
      "1\n",
      "LogLikelihood for the training -850.7155724932923\n",
      "LogLikelihood for the test -435.82508918702626\n",
      "LogLikelihood for the valid -205.35012846818123\n",
      "2\n",
      "LogLikelihood for the training -472.9591145424052\n",
      "LogLikelihood for the test -147.36562421716297\n",
      "LogLikelihood for the valid -172.29422435326995\n",
      "3\n",
      "LogLikelihood for the training -1209.1141163050038\n",
      "LogLikelihood for the test -147.35373995065441\n",
      "LogLikelihood for the valid -170.33353095258886\n",
      "4\n",
      "LogLikelihood for the training -960.5925479466252\n",
      "LogLikelihood for the test -568.9308337290037\n",
      "LogLikelihood for the valid -60.834758635104286\n",
      "5\n",
      "LogLikelihood for the training -944.8847146290675\n",
      "LogLikelihood for the test -295.3558423586867\n",
      "LogLikelihood for the valid -60.83223612231184\n",
      "6\n",
      "LogLikelihood for the training -702.3065097198328\n",
      "LogLikelihood for the test -147.30234664711122\n",
      "LogLikelihood for the valid -207.49090955360808\n",
      "7\n",
      "LogLikelihood for the training -922.9041743877042\n",
      "LogLikelihood for the test -734.8114793212753\n",
      "LogLikelihood for the valid -60.82725946177887\n",
      "8\n",
      "LogLikelihood for the training -1705.1646548218052\n",
      "LogLikelihood for the test -147.26054087778564\n",
      "LogLikelihood for the valid -171.43804978589156\n",
      "9\n",
      "LogLikelihood for the training -537.5649900302268\n",
      "LogLikelihood for the test -437.8175333178656\n",
      "LogLikelihood for the valid -96.55928042403536\n",
      "10\n",
      "LogLikelihood for the training -1238.3801006769095\n",
      "LogLikelihood for the test -147.21448735210302\n",
      "LogLikelihood for the valid -94.65018012447008\n",
      "11\n",
      "LogLikelihood for the training -588.6008437074253\n",
      "LogLikelihood for the test -526.0971099544491\n",
      "LogLikelihood for the valid -94.85662286637668\n",
      "12\n",
      "LogLikelihood for the training -1545.9826211967793\n",
      "LogLikelihood for the test -575.8992082941688\n",
      "LogLikelihood for the valid -60.808045055891206\n",
      "13\n",
      "LogLikelihood for the training -1190.0215446735174\n",
      "LogLikelihood for the test -591.388146543406\n",
      "LogLikelihood for the valid -60.80354168308524\n",
      "14\n",
      "LogLikelihood for the training -1083.0663159348414\n",
      "LogLikelihood for the test -298.4085953969367\n",
      "LogLikelihood for the valid -60.7992101889982\n",
      "15\n",
      "LogLikelihood for the training -430.2724367868169\n",
      "LogLikelihood for the test -441.5206146288103\n",
      "LogLikelihood for the valid -60.794883434437104\n",
      "16\n",
      "LogLikelihood for the training -906.892101996229\n",
      "LogLikelihood for the test -147.04639804915237\n",
      "LogLikelihood for the valid -174.01163931844152\n",
      "17\n",
      "LogLikelihood for the training -1421.3322781939162\n",
      "LogLikelihood for the test -147.01469207425737\n",
      "LogLikelihood for the valid -401.0412607435817\n",
      "18\n",
      "LogLikelihood for the training -1553.3298905214488\n",
      "LogLikelihood for the test -741.4918355825273\n",
      "LogLikelihood for the valid -60.77910626225403\n",
      "19\n",
      "LogLikelihood for the training -466.9716535796333\n",
      "LogLikelihood for the test -293.7950023515665\n",
      "LogLikelihood for the valid -169.71772696534512\n",
      "20\n",
      "LogLikelihood for the training -1412.0743412428315\n",
      "LogLikelihood for the test -146.9166363519763\n",
      "LogLikelihood for the valid -394.0636842473863\n",
      "21\n",
      "LogLikelihood for the training -474.0388740210656\n",
      "LogLikelihood for the test -296.09514761207447\n",
      "LogLikelihood for the valid -60.762166342218904\n",
      "22\n",
      "LogLikelihood for the training -912.411473722306\n",
      "LogLikelihood for the test -303.1924275883955\n",
      "LogLikelihood for the valid -171.66982357799932\n",
      "23\n",
      "LogLikelihood for the training -730.0023036131984\n",
      "LogLikelihood for the test -334.25766089562035\n",
      "LogLikelihood for the valid -203.84058243467342\n",
      "24\n",
      "LogLikelihood for the training -1311.2108270230276\n",
      "LogLikelihood for the test -590.5018384277262\n",
      "LogLikelihood for the valid -169.5097279202113\n",
      "25\n",
      "LogLikelihood for the training -1785.2730170240368\n",
      "LogLikelihood for the test -146.73922536874446\n",
      "LogLikelihood for the valid -206.78182757357493\n",
      "26\n",
      "LogLikelihood for the training -507.87814029427625\n",
      "LogLikelihood for the test -295.3966476709792\n",
      "LogLikelihood for the valid -170.99720028852857\n",
      "27\n",
      "LogLikelihood for the training -935.2964349137517\n",
      "LogLikelihood for the test -296.0010416894287\n",
      "LogLikelihood for the valid -170.07787059919946\n",
      "28\n",
      "LogLikelihood for the training -581.9785034164281\n",
      "LogLikelihood for the test -297.9442051038269\n",
      "LogLikelihood for the valid -170.55290465514094\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "ma=StochasticGradientAscent(training_samples,100,10,5,30,vec,alpha=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditionalDPP(LowRankMatrix,itemsObserved,nb_items,nb_trait):\n",
    "    \n",
    "    M=nb_items\n",
    "    itemsObservedSet=set(itemsObserved)\n",
    "    K=nb_trait\n",
    "    \n",
    "    itemsSet=set(np.linspace(0,9,num=10,dtype=int))\n",
    "    \n",
    "    allItemsNotObserved=list(itemsSet.difference(itemsObservedSet))\n",
    "    a_bar=len(allItemsNotObserved)\n",
    "    a=len(itemsObservedSet)\n",
    "    \n",
    "    V_a=LowRankMatrix[itemsObserved,:]\n",
    "    V_a_bar=LowRankMatrix[allItemsNotObserved,:]\n",
    "    \n",
    "    inverseVA=lg.pinv(V_a@(V_a.T))\n",
    "    \n",
    "    Z_normalize=np.identity(K)-((V_a.T)@inverseVA)@V_a\n",
    "    \n",
    "    L_a=(V_a_bar@Z_normalize)@(V_a_bar.T)\n",
    "    \n",
    "    vector_ind=np.zeros((nb_items,1))\n",
    "    \n",
    "    ind=0\n",
    "    for item in allItemsNotObserved:\n",
    "        vector_ind[item]=ind\n",
    "        ind+=1\n",
    "    \n",
    "    LowRankConditionedOnItemsObserved=V_a_bar@Z_normalize\n",
    "    \n",
    "        \n",
    "    \n",
    "    return L_a, LowRankConditionedOnItemsObserved, vector_ind,allItemsNotObserved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(LowRankMatrix,itemsObserved,nb_items,nb_trait):\n",
    "    probsPrediction=np.zeros((nb_items,1))\n",
    "    L_a,LowRankConditionedOnItemsObserved,vector_ind,allItemsNotObserved=conditionalDPP(LowRankMatrix,itemsObserved,nb_items,nb_trait)\n",
    "    \n",
    "    for item in allItemsNotObserved:\n",
    "        RowColInd=vector_ind[item]\n",
    "        #print(RowColInd)\n",
    "        probsPrediction[item]=L_a[int(RowColInd),int(RowColInd)]\n",
    "        \n",
    "    nextItem=np.argmax(probsPrediction)\n",
    "    \n",
    "    return nextItem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemsObserved=[2,5,6,8]\n",
    "\n",
    "nxt=prediction(ma,itemsObserved,10,5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(nxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
